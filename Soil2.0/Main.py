import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.stats import chi2_contingency, f_oneway
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import seaborn as sns
import matplotlib.pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from factor_analyzer import calculate_kmo
from pathlib import Path
from factor_analyzer import FactorAnalyzer
import requests
import streamlit as st
import shap

class DataPipeline:
    def __init__(self, input_file, output_file):
        self.input_file = input_file
        self.output_file = output_file
        self.territorial_vars = ['CardID', 'RUREG', 'LAT', 'LONG', 'ALT']
        self.territorial_data = None
        self.data = None
        self.artifacts = {}
        self.significance_analyzer = None
        self.multicollinearity_processor = None
        self.kmo_checker = None
        self.factor_analyzer = None
        self.factor_name_generator = None

    def run(self):
        # –≠—Ç–∞–ø 0: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        raw_data = pd.read_excel(self.input_file)
        self.territorial_data = raw_data[self.territorial_vars]
        self.data = raw_data.drop(columns=self.territorial_vars)

        # –≠—Ç–∞–ø 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        preprocessor = SoilPreprocessor(self.data)
        self.data, self.artifacts['encoding_map'] = preprocessor.process()
        print("\n[–≠—Ç–∞–ø 1] –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

        # –≠—Ç–∞–ø 2: –û—Ü–µ–Ω–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
        self.significance_analyzer = SignificanceAnalyzer(
            target_column='SOIL_ID',
            exclude_columns=self.territorial_vars  # –ü–µ—Ä–µ–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        )
        self.data, self.artifacts['feature_importances'] = self.significance_analyzer.process(self.data)
        print("\n[–≠—Ç–∞–ø 2] –ê–Ω–∞–ª–∏–∑ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("–¢–æ–ø-5 –∑–Ω–∞—á–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        print(self.artifacts['feature_importances'].head(5))
        print(f"R2 –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {max(r['R2'] for r in self.significance_analyzer.results):.2f}")

        # –≠—Ç–∞–ø 3: –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å
        self.multicollinearity_processor = MulticollinearityProcessor()
        self.data, self.artifacts['multicollinearity'] = self.multicollinearity_processor.process(self.data)
        print("\n[–≠—Ç–∞–ø 3] –û–±—Ä–∞–±–æ—Ç–∫–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"–£–¥–∞–ª–µ–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.artifacts['multicollinearity']['removed_features'])}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π VIF –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.artifacts['multicollinearity']['vif_report']['VIF'].max():.1f}")

        # –≠—Ç–∞–ø 4: –û—Ü–µ–Ω–∫–∞ KMO
        self.kmo_checker = KMOChecker()
        self.data, self.artifacts['kmo'] = self.kmo_checker.process(self.data)
        print("\n[–≠—Ç–∞–ø 4] KMO –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"–£–¥–∞–ª–µ–Ω–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –Ω–∏–∑–∫–∏–º KMO: {len(self.artifacts['kmo']['removed_features'])}")
        print(f"–ò—Ç–æ–≥–æ–≤—ã–π KMO: {self.artifacts['kmo']['kmo_model']:.2f}")

        # –≠—Ç–∞–ø 5: –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        self.factor_analyzer = FactorAnalyzerProcessor()
        artifacts_factor = self.factor_analyzer.process(self.data, self.output_file)
        self.artifacts['factor_analysis'] = artifacts_factor
        print("\n[–≠—Ç–∞–ø 5] –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

        # –≠—Ç–∞–ø 6: –§–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        self.factor_score_processor = FactorScoreProcessor()
        artifacts_factor_scores = self.factor_score_processor.process(self.data, self.artifacts['factor_analysis']['loadings'])
        self.artifacts['factor_scores'] = artifacts_factor_scores
        print("\n[–≠—Ç–∞–ø 6] –§–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã!")

        # –≠—Ç–∞–ø 7: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—Å—Ç–∞ "–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ"
        self.factor_name_processor = FactorNameProcessor()
        artifacts_factor_names = self.factor_name_processor.process(self.artifacts['factor_analysis']['loadings'])
        self.artifacts['factor_names'] = artifacts_factor_names
        print("\n[–≠—Ç–∞–ø 7] –õ–∏—Å—Ç '–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ' –¥–æ–±–∞–≤–ª–µ–Ω!")

        # –≠—Ç–∞–ø 8: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        self.factor_name_generator = FactorNameGenerator()
        variables_df = self.artifacts['factor_names']
        communalities_df = self.artifacts['factor_analysis']['communalities']
        self.artifacts['factor_names'] = self.factor_name_generator.process(variables_df, communalities_df)
        print("\n[–≠—Ç–∞–ø 8] –ù–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!")

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_results()
        print("\n–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:", len(self.data.columns))

    def save_results(self):
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤
        report_dir = Path("–û—Ç—á–µ—Ç—ã –∏ –≤—ã–≤–æ–¥—ã")
        report_dir.mkdir(exist_ok=True)
        output_path = report_dir / self.output_file

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if self.territorial_data is not None:
            main_data = pd.concat([
                self.territorial_data.reset_index(drop=True),
                self.data.reset_index(drop=True)
            ], axis=1)
        else:
            main_data = self.data.copy()

        try:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            main_data = pd.concat([
                self.territorial_data.reset_index(drop=True),
                self.data.reset_index(drop=True)
            ], axis=1)

            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                main_data.to_excel(writer, sheet_name="–î–∞–Ω–Ω—ã–µ", index=False)

                # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if 'factor_analysis' in self.artifacts:
                    self.artifacts['factor_analysis']['loadings'].to_excel(
                        writer, sheet_name="–§–∞–∫—Ç–æ—Ä–Ω—ã–µ –Ω–∞–≥—Ä—É–∑–∫–∏", index_label="–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è"
                    )
                    self.artifacts['factor_analysis']['communalities'].to_excel(
                        writer, sheet_name="–û–±—â–Ω–æ—Å—Ç–∏", index_label="–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è"
                    )

                # –§–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if 'factor_scores' in self.artifacts:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫ —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–º –æ—Ü–µ–Ω–∫–∞–º
                    factor_scores_with_territory = pd.concat([
                        self.territorial_data.reset_index(drop=True),
                        self.artifacts['factor_scores'].reset_index(drop=True)
                    ], axis=1)

                    factor_scores_with_territory.to_excel(
                        writer, sheet_name="–§–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏", index_label="–ò–Ω–¥–µ–∫—Å"
                    )

                # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ —Ñ–∞–∫—Ç–æ—Ä–∞–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if 'factor_names' in self.artifacts:
                    self.artifacts['factor_names'].to_excel(
                        writer, sheet_name="–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ", index=False
                    )

                # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if 'factor_names' in self.artifacts:
                    self.artifacts['factor_names'].to_excel(
                        writer, sheet_name="–ù–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤", index=False
                    )

        except FileNotFoundError:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –µ–≥–æ –∑–∞–Ω–æ–≤–æ
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                pd.DataFrame().to_excel(writer, sheet_name="–î–∞–Ω–Ω—ã–µ")
            self.save_results()  # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–≤—Ç–æ—Ä—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ

        # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –≤ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞—Ö
        if 'factor_names' in self.artifacts:
            factor_names = self.artifacts['factor_names']
            if '–û—Ç–≤–µ—Ç' in factor_names.columns:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å—Ç–æ–ª–±—Ü–∞
                name_mapping = dict(zip(factor_names['–§–∞–∫—Ç–æ—Ä'], factor_names['–û—Ç–≤–µ—Ç']))
            else:
                st.warning("–°—Ç–æ–ª–±–µ—Ü '–û—Ç–≤–µ—Ç' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ factor_names. –ù–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")
                name_mapping = {}

            if 'factor_analysis' in self.artifacts:
                self.artifacts['factor_analysis']['loadings'].rename(columns=name_mapping, inplace=True)
            if 'factor_scores' in self.artifacts:
                self.artifacts['factor_scores'].rename(columns=name_mapping, inplace=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV-–æ—Ç—á–µ—Ç—ã
        reports = {
            'model_comparison.csv': pd.DataFrame(self.significance_analyzer.results)
            if self.significance_analyzer is not None  # –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None
               and hasattr(self, 'significance_analyzer')
            else pd.DataFrame(),
            'feature_importances.csv': self.artifacts.get('feature_importances', pd.DataFrame()),
            'vif_report.csv': self.artifacts.get('multicollinearity', {}).get('vif_report', pd.DataFrame()),
            'low_kmo_features.csv': pd.DataFrame(
                {'features': self.artifacts.get('kmo', {}).get('removed_features', [])}),
            'final_correlation.csv': self.artifacts.get('kmo', {}).get('correlation_matrix', pd.DataFrame()),
            'factor_loadings.csv': self.artifacts.get('factor_analysis', {}).get('loadings', pd.DataFrame()),
            'communalities.csv': self.artifacts.get('factor_analysis', {}).get('communalities', pd.DataFrame()),
            'factor_scores.csv': self.artifacts.get('factor_scores', pd.DataFrame()),
            'factor_names.csv': self.artifacts.get('factor_names', pd.DataFrame())
        }

        for filename, data in reports.items():
            file_path = report_dir / filename
            if isinstance(data, pd.DataFrame) and not data.empty:
                data.to_csv(file_path, index=False, encoding='utf-8-sig')

        print(f"\n–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")

    def run_stage(self, stage_num):
        if stage_num == 1:
            # –≠—Ç–∞–ø 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Ä–∞–∑–¥–µ–ª—è–µ–º –¥–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
            raw_data = pd.read_excel(self.input_file)
            self.territorial_data = raw_data[self.territorial_vars]

            # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            preprocessor = SoilPreprocessor(raw_data.drop(columns=self.territorial_vars))

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É
            self.data, self.artifacts['encoding_map'] = preprocessor.process()
            preprocessor.save_results()

        elif stage_num == 2:
            # –≠—Ç–∞–ø 2: –û—Ü–µ–Ω–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
            self.significance_analyzer = SignificanceAnalyzer()
            self.data, self.artifacts['feature_importances'] = self.significance_analyzer.process(self.data)

        elif stage_num == 3:
            # –≠—Ç–∞–ø 3: –ú—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å
            self.multicollinearity_processor = MulticollinearityProcessor()
            self.data, self.artifacts['multicollinearity'] = self.multicollinearity_processor.process(self.data)

        elif stage_num == 4:
            # –≠—Ç–∞–ø 4: KMO –∞–Ω–∞–ª–∏–∑
            self.kmo_checker = KMOChecker()
            self.data, self.artifacts['kmo'] = self.kmo_checker.process(self.data)

        elif stage_num == 5:
            # –≠—Ç–∞–ø 5: –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            self.factor_analyzer = FactorAnalyzerProcessor()
            self.artifacts['factor_analysis'] = self.factor_analyzer.process(self.data, self.output_file)

        elif stage_num == 6:
            # üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            if 'factor_analysis' not in self.artifacts:
                raise ValueError(
                    "‚ö†Ô∏è –û—à–∏–±–∫–∞: –≠—Ç–∞–ø 6 –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω, —Ç–∞–∫ –∫–∞–∫ —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–≠—Ç–∞–ø 5) –Ω–µ –±—ã–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à—ë–Ω.")

            # –≠—Ç–∞–ø 6: –§–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            self.factor_score_processor = FactorScoreProcessor()
            self.artifacts['factor_scores'] = self.factor_score_processor.process(
                self.data, self.artifacts['factor_analysis']['loadings']
            )

        elif stage_num == 7:
            # –≠—Ç–∞–ø 7: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            self.factor_name_processor = FactorNameProcessor()
            self.artifacts['factor_names'] = self.factor_name_processor.process(
                self.artifacts['factor_analysis']['loadings']
            )

        elif stage_num == 8:
            # –≠—Ç–∞–ø 8: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π
            self.factor_name_generator = FactorNameGenerator()
            variables_df = self.artifacts['factor_names']
            communalities_df = self.artifacts['factor_analysis']['communalities']
            self.artifacts['factor_names'] = self.factor_name_generator.process(variables_df, communalities_df)

        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —ç—Ç–∞–ø –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stage_num}")

        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞
        self.save_results()

class SoilPreprocessor:
    def __init__(self, df, target_column='SOIL_ID', threshold_ratio=0.1):
        self.df = df.copy()
        self.target_column = target_column
        self.threshold = threshold_ratio * len(self.df)
        self.encoding_map = {}
        self.scaler = StandardScaler()
        self.original_numeric_columns = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
        self.encoded_columns = []  # –°–ø–∏—Å–æ–∫ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
        self.processed_df = None
        self.report_dir = Path("–û—Ç—á–µ—Ç—ã –∏ –≤—ã–≤–æ–¥—ã")
        self.report_dir.mkdir(exist_ok=True)

    @staticmethod
    def evaluate_column_significance(df, column, target_column):
        if np.issubdtype(df[column].dtype, np.number):
            if target_column in df.columns:
                groups = [group.dropna() for _, group in df.groupby(target_column)[column]]
                if len(groups) > 1:
                    _, p_value = f_oneway(*groups)
                    return p_value
        else:
            if target_column in df.columns:
                contingency_table = pd.crosstab(df[column].dropna(), df[target_column].dropna())
                if contingency_table.size > 0:
                    _, p_value, _, _ = chi2_contingency(contingency_table)
                    return p_value
        return None

    def _reorder_columns(self):
        categorical = self.df.select_dtypes(exclude='number').columns.tolist()
        numerical = self.df.select_dtypes(include='number').columns.tolist()
        self.df = self.df[categorical + numerical]

    def _filter_columns(self):
        columns_to_remove = [col for col in self.df.columns if self.df[col].count() < self.threshold]
        significant_columns = []

        for col in columns_to_remove:
            p_value = self.evaluate_column_significance(self.df, col, self.target_column)
            if p_value is not None and p_value < 0.05:
                significant_columns.append(col)

        self.df = self.df.drop(columns=[col for col in columns_to_remove if col not in significant_columns])

    def _replace_missing(self):
        for col in self.df.columns:
            if col == self.target_column:
                continue

            if np.issubdtype(self.df[col].dtype, np.number):
                self.df[col] = self.df.groupby(self.target_column)[col].transform(
                    lambda x: x.fillna(x.mean() if x.isnull().mean() < 0.5 else x.median()))
            else:
                self.df[col] = self.df.groupby(self.target_column)[col].transform(
                    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))

    def _encode_text_columns(self):
        text_columns = self.df.select_dtypes(exclude='number').columns.difference([self.target_column])
        self.encoded_columns = list(text_columns)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤

        for col in text_columns:
            unique_values = self.df[col].astype(str).unique()
            encoding_dict = {value: idx for idx, value in enumerate(sorted(unique_values))}
            self.df[col] = self.df[col].astype(str).map(encoding_dict)
            self.encoding_map[col] = encoding_dict

    def _normalize(self):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã (–∏—Å–∫–ª—é—á–∞—è —Ü–µ–ª–µ–≤—ã–µ –∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
        exclude = [self.target_column] + self.encoded_columns
        self.original_numeric_columns = self.df.select_dtypes(include='number').columns.difference(exclude)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ö–æ–¥–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
        if not self.original_numeric_columns.empty:
            self.df[self.original_numeric_columns] = self.scaler.fit_transform(self.df[self.original_numeric_columns])

    def process(self):
        self._reorder_columns()
        self._filter_columns()
        self._replace_missing()
        self._encode_text_columns()
        self._normalize()
        self.processed_df = self.df
        return self.processed_df, self.encoding_map

    def save_results(self, output_excel="—ç—Ç–∞–ø 1 –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞.xlsx", output_txt="encoding_mapping.txt"):
        # –ò–∑–º–µ–Ω—è–µ–º –ø—É—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        output_excel = self.report_dir / output_excel
        output_txt = self.report_dir / output_txt

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ Excel —Ñ–∞–π–ª–∞
        self.processed_df.to_excel(output_excel, index=False)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π
        with open(output_txt, 'w', encoding='utf-8') as f:
            f.write("–ü–æ–ª–Ω–∞—è —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n\n")
            for column, mapping in self.encoding_map.items():
                f.write(f"–°—Ç–æ–ª–±–µ—Ü: {column}\n")
                sorted_mapping = sorted(mapping.items(), key=lambda x: x[1])
                for text, code in sorted_mapping:
                    f.write(f"  '{text}' ‚Üí {code}\n")
                f.write("-" * 50 + "\n")

        print("\n–≠—Ç–∞–ø –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∞–ø–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_excel}")
        print(f"–§–∞–π–ª —Å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º: {output_txt}")

class SignificanceAnalyzer:
    def __init__(self, target_column='SOIL_ID', importance_threshold=0.01, exclude_columns = None):
        self.target_column = target_column
        self.threshold = importance_threshold
        self.exclude_columns = exclude_columns or []
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ LAT
        self.exclude_columns.extend(['LAT', 'LONG', 'ALT', 'CardID', 'RUREG'])
        self.results = []
        # –ù–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è feature_importances –ø–æ –º–æ–¥–µ–ª—è–º
        self.feature_importances_dict = {}
        self.best_model = None

    def process(self, df):
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∏—Å–∫–ª—é—á–∞–µ–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        valid_exclude = [col for col in self.exclude_columns if col in df.columns]
        X = df.drop(columns=[self.target_column] + valid_exclude)
        y = df[self.target_column]

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        X_filled = X.fillna(0)

        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
        models = {
            "Random Forest": RandomForestRegressor(n_estimators=95, random_state=42),
            "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
            "Support Vector Regressor (SVR)": SVR(kernel='rbf', C=100, gamma='scale'),
            "K-Nearest Neighbors (KNN)": KNeighborsRegressor(n_neighbors=5)
        }

        self.results = []
        for model_name, model in models.items():
            model.fit(X_filled, y)
            y_pred = model.predict(X_filled)

            mae = mean_absolute_error(y, y_pred)
            mse = mean_squared_error(y, y_pred)
            r2 = r2_score(y, y_pred)

            self.results.append({
                "Model": model_name,
                "MAE": mae,
                "MSE": mse,
                "R2": r2
            })

            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç feature_importances, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—ë
            if hasattr(model, "feature_importances_"):
                fi_df = pd.DataFrame({
                    'Feature': X.columns,
                    'Importance': model.feature_importances_
                }).sort_values('Importance', ascending=False)
                self.feature_importances_dict[model_name] = fi_df

        results_df = pd.DataFrame(self.results).sort_values(by="R2", ascending=False)
        best_model_name = results_df.iloc[0]["Model"]
        self.best_model = models[best_model_name]

        # –¢–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π DataFrame –∏ —Å–ª–æ–≤–∞—Ä—å —Å –≤–∞–∂–Ω–æ—Å—Ç—è–º–∏
        return df, self.feature_importances_dict

    def _save_feature_importance_plot(self, model_name):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ PNG —Ñ–∞–π–ª"""
        plt.figure(figsize=(10, 6))
        sns.barplot(
            data=self.feature_importances_dict[model_name].head(10),
            x="Importance",
            y="Feature",
            palette="viridis"
        )
        plt.title(f"–¢–æ–ø-10 –∑–Ω–∞—á–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ({model_name})")
        plt.xlabel("Importance")
        plt.ylabel("Feature")
        plt.tight_layout()
        plot_path = Path("–û—Ç—á–µ—Ç—ã –∏ –≤—ã–≤–æ–¥—ã") / f"significant_top_10_features_{model_name}.png"
        plt.savefig(plot_path)
        plt.close()


class MulticollinearityProcessor:
    def __init__(self, vif_threshold=5):
        self.vif_threshold = vif_threshold
        self.vif_data = None
        self.removed_features = []

    def process(self, df):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏
        missing_data = df.isna()

        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –º–µ–¥–∏–∞–Ω–æ–π
        df_filled = df.apply(lambda x: x.fillna(x.median()) if np.issubdtype(x.dtype, np.number) else x)

        # –†–∞—Å—á–µ—Ç VIF
        df_with_const = add_constant(df_filled)
        vif_df = pd.DataFrame()
        vif_df["Variable"] = df_with_const.columns
        vif_df["VIF"] = [variance_inflation_factor(df_with_const.values, i)
                         for i in range(df_with_const.shape[1])]

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        high_vif = vif_df[(vif_df["VIF"] > self.vif_threshold) & (vif_df["Variable"] != "const")]
        self.removed_features = high_vif["Variable"].tolist()

        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df_filtered = df_filled.drop(columns=self.removed_features)

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏
        df_filtered[missing_data] = np.nan

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
        self.vif_data = vif_df

        return df_filtered, {
            "vif_report": vif_df,
            "removed_features": self.removed_features
        }
class KMOChecker:
    def __init__(self, correlation_threshold=0.3, kmo_threshold=0.5):
        self.correlation_threshold = correlation_threshold
        self.kmo_threshold = kmo_threshold
        self.low_kmo_vars = []
        self.final_correlation_matrix = None
        self.report_dir = Path("–û—Ç—á–µ—Ç—ã –∏ –≤—ã–≤–æ–¥—ã")
        self.report_dir.mkdir(exist_ok=True)

    @staticmethod
    def is_positive_definite(matrix):
        try:
            np.linalg.cholesky(matrix)
            return True
        except np.linalg.LinAlgError:
            return False

    def _save_correlation_plots(self, correlation_matrix):
        num_groups = 4
        group_size = len(correlation_matrix) // num_groups
        for i in range(num_groups):
            plt.figure(figsize=(8, 6))
            start = i * group_size
            end = (i + 1) * group_size if i != num_groups - 1 else len(correlation_matrix)
            group_corr = correlation_matrix.iloc[start:end, start:end]
            sns.heatmap(group_corr, annot=False, cmap="coolwarm", fmt='.2f', linewidths=0.5)
            plt.title(f"–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (–ì—Ä—É–ø–ø–∞ {i + 1})")
            plot_path = self.report_dir / f"correlation_group_{i + 1}.png"
            plt.savefig(plot_path)
            plt.close()

    def process(self, df):
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ KMO.py
        numerical_df = df.select_dtypes(include='number')
        categorical_df = df.select_dtypes(exclude='number')

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        correlation_matrix = numerical_df.corr()
        correlation_mask = correlation_matrix.abs() >= self.correlation_threshold
        filtered_columns = correlation_matrix.columns[correlation_mask.any(axis=0)].tolist()
        numerical_filtered = numerical_df[filtered_columns]

        # –ó–∞–º–µ–Ω–∞ NaN
        numerical_filled = numerical_filtered.apply(lambda x: x.fillna(x.median()))

        # –†–∞—Å—á–µ—Ç KMO
        corr_values = numerical_filled.corr().values
        corr_values[np.isnan(corr_values)] = 0
        kmo_all, kmo_model = calculate_kmo(corr_values)

        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –Ω–∏–∑–∫–∏–º KMO
        self.low_kmo_vars = numerical_filled.columns[np.where(kmo_all < self.kmo_threshold)[0]].tolist()
        numerical_final = numerical_filled.drop(columns=self.low_kmo_vars)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã
        final_corr_matrix = numerical_final.corr()
        if not self.is_positive_definite(final_corr_matrix):
            raise ValueError("–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –Ω–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞!")

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._save_correlation_plots(final_corr_matrix)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
        pd.DataFrame({'–ü—Ä–∏–∑–Ω–∞–∫–∏': self.low_kmo_vars}).to_csv(
            self.report_dir / "low_kmo_features.csv", index=False, encoding='utf-8-sig'
        )
        pd.DataFrame({'KMO': [kmo_model]}).to_csv(
            self.report_dir / "kmo_score.csv", index=False
        )

        # –ò—Ç–æ–≥–æ–≤—ã–π DataFrame
        final_df = pd.concat([categorical_df, numerical_final], axis=1)
        self.final_correlation_matrix = final_corr_matrix

        return final_df, {
            "kmo_model": kmo_model,
            "removed_features": self.low_kmo_vars,
            "correlation_matrix": self.final_correlation_matrix
        }
class FactorAnalyzerProcessor:
    def __init__(self, naming_file="naming_of_variables.xlsx"):
        self.report_dir = Path("–û—Ç—á–µ—Ç—ã –∏ –≤—ã–≤–æ–¥—ã")
        self.report_dir.mkdir(exist_ok=True)  # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        self.naming_file_path = self.report_dir / naming_file

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏ —Å–æ–∑–¥–∞–µ–º –µ–≥–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        if not self.naming_file_path.exists():
            default_data = pd.DataFrame({
                "–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è": [],
                "–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö": []
            })
            default_data.to_excel(self.naming_file_path, index=False)
            st.warning(f"–§–∞–π–ª {naming_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–Ω –ø—É—Å—Ç–æ–π —à–∞–±–ª–æ–Ω.")

        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            self.naming_df = pd.read_excel(self.naming_file_path)
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {naming_file}: {str(e)}")
            self.naming_df = pd.DataFrame()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Ç–µ–π –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        self.scree_plot_path = self.report_dir / "scree_plot.png"
        self.parallel_plot_path = self.report_dir / "parallel_analysis.png"
        self.variance_plot_path = self.report_dir / "explained_variance.png"

    def _save_plots(self, eigenvalues, mean_random_eigenvalues, cumulative_variance_ratio):
        # Scree Plot
        plt.figure(figsize=(8, 6))
        plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='--')
        plt.title("Scree Plot")
        plt.xlabel("–ù–æ–º–µ—Ä –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã")
        plt.ylabel("–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ")
        plt.axhline(y=1, color='r', linestyle='-', label="–ö—Ä–∏—Ç–µ—Ä–∏–π –ö–∞–π–∑–µ—Ä–∞")
        plt.grid()
        plt.legend()
        plt.savefig(self.scree_plot_path)
        plt.close()

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        plt.figure(figsize=(8, 6))
        plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', label="–†–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
        plt.plot(range(1, len(mean_random_eigenvalues) + 1), mean_random_eigenvalues,
                marker='x', label="–°–ª—É—á–∞–π–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
        plt.title("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        plt.xlabel("–ù–æ–º–µ—Ä –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã")
        plt.ylabel("–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ")
        plt.legend()
        plt.grid()
        plt.savefig(self.parallel_plot_path)
        plt.close()

        # –û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        plt.figure(figsize=(8, 6))
        plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o')
        plt.axhline(y=0.7, color='r', linestyle='--', label="70% –¥–∏—Å–ø–µ—Ä—Å–∏–∏")
        plt.title("–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –æ–±—ä—è—Å–Ω—ë–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è")
        plt.xlabel("–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã")
        plt.ylabel("–î–∏—Å–ø–µ—Ä—Å–∏—è")
        plt.legend()
        plt.grid()
        plt.savefig(self.variance_plot_path)
        plt.close()

    def _create_empty_sheets(self, output_path):
        """–°–æ–∑–¥–∞–µ—Ç –ø—É—Å—Ç—ã–µ –ª–∏—Å—Ç—ã –≤ —Ñ–∞–π–ª–µ, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç."""
        if not output_path.exists():
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª —Å –ø—É—Å—Ç—ã–º–∏ –ª–∏—Å—Ç–∞–º–∏
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                pd.DataFrame().to_excel(writer, sheet_name="–§–∞–∫—Ç–æ—Ä–Ω—ã–µ –Ω–∞–≥—Ä—É–∑–∫–∏", index=False)
                pd.DataFrame().to_excel(writer, sheet_name="–û–±—â–Ω–æ—Å—Ç–∏", index=False)

    def process(self, df, output_file, num_factors=None):
        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –∏–∑ FactorAnalys.py
        corr_matrix = df.corr().fillna(0)
        eigenvalues, _ = np.linalg.eig(corr_matrix)

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        random_eigenvalues = []
        for _ in range(100):
            random_data = np.random.normal(size=df.shape)
            rand_eigvals, _ = np.linalg.eig(np.corrcoef(random_data, rowvar=False))
            random_eigenvalues.append(rand_eigvals)

        mean_random_eigenvalues = np.mean(random_eigenvalues, axis=0)
        explained_variance = eigenvalues / eigenvalues.sum()
        cumulative_variance = np.cumsum(explained_variance)

        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –º–∞—Å—Å–∏–≤–æ–≤
        min_len = min(len(eigenvalues), len(mean_random_eigenvalues))
        eigenvalues_trunc = eigenvalues[:min_len]
        mean_random_trunc = mean_random_eigenvalues[:min_len]

        # –†–∞—Å—á–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        num_factors_kaiser = np.sum(eigenvalues_trunc > 1)
        num_factors_parallel = np.sum(eigenvalues_trunc > mean_random_trunc)
        num_factors_variance = np.argmax(cumulative_variance >= 0.7) + 1

        # –í—ã–≤–æ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –≤ –∫–æ–Ω—Å–æ–ª—å (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ)
        print("\n[–≠—Ç–∞–ø 5] –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∞–∫—Ç–æ—Ä–æ–≤:")
        print(f"‚Ä¢ –ü–æ –∫—Ä–∏—Ç–µ—Ä–∏—é –ö–∞–π–∑–µ—Ä–∞: {num_factors_kaiser}")
        print(f"‚Ä¢ –ü–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É: {num_factors_parallel}")
        print(f"‚Ä¢ –î–ª—è 70% –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏: {num_factors_variance}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
        self._save_plots(eigenvalues, mean_random_eigenvalues, cumulative_variance)

        # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, 3)
        if num_factors is None:
            num_factors = 3

        # –§–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        fa = FactorAnalyzer(n_factors=num_factors, rotation='varimax')
        fa.fit(df.fillna(df.median()))
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (eigenvalues)
        eigenvalues = fa.get_eigenvalues()[0]
        # –î–æ–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –∫–∞–∂–¥—ã–º —Ñ–∞–∫—Ç–æ—Ä–æ–º
        explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
        # –°–æ–≤–æ–∫—É–ø–Ω–∞—è –¥–æ–ª—è –æ–±—ä—è—Å–Ω–µ–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏ (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
        cumulative_variance = np.cumsum(explained_variance_ratio) * 100
        print(f"–û–±—ä—è—Å–Ω–µ–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏: {cumulative_variance[-1]:.2f}%")
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        loadings = pd.DataFrame(
            fa.loadings_,
            index=df.columns,
            columns=[f"–§–∞–∫—Ç–æ—Ä {i + 1}" for i in range(num_factors)]
        )
        communalities = pd.DataFrame(
            fa.get_communalities(),
            index=df.columns,
            columns=["–û–±—â–Ω–æ—Å—Ç—å"]
        ).assign(–û–ø–∏—Å–∞–Ω–∏–µ=df.columns.map(
            dict(zip(self.naming_df['–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è'], self.naming_df['–û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö']))
        ))

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª
        output_path = self.report_dir / output_file
        self._create_empty_sheets(output_path)  # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ –ª–∏—Å—Ç—ã, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç

        with pd.ExcelWriter(output_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
            loadings.to_excel(writer, sheet_name="–§–∞–∫—Ç–æ—Ä–Ω—ã–µ –Ω–∞–≥—Ä—É–∑–∫–∏", index_label="–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è")
            communalities.to_excel(writer, sheet_name="–û–±—â–Ω–æ—Å—Ç–∏", index_label="–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è")

        return {
            "num_factors_kaiser": num_factors_kaiser,
            "num_factors_parallel": num_factors_parallel,
            "num_factors_variance": num_factors_variance,
            "loadings": loadings,
            "communalities": communalities
        }

class FactorScoreProcessor:
    def process(self, df, factor_loadings):
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —á–∏—Å–ª–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        assert df.shape[1] == factor_loadings.shape[0], "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–∞–∫—Ç–æ—Ä–Ω—ã—Ö –Ω–∞–≥—Ä—É–∑–æ–∫ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç!"

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
        n_factors = factor_loadings.shape[1]

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        fa = FactorAnalyzer(n_factors=n_factors, rotation=None, method="principal")
        fa.fit(df)

        # –†–∞—Å—á–µ—Ç —Ñ–∞–∫—Ç–æ—Ä–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
        factor_scores = fa.transform(df)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤ DataFrame
        factor_scores_df = pd.DataFrame(
            factor_scores,
            columns=[f"–§–∞–∫—Ç–æ—Ä {i+1}" for i in range(n_factors)],
            index=df.index
        )

        return factor_scores_df
class FactorNameProcessor:
    def process(self, factor_loadings, threshold=0.4):
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ —Ñ–∞–∫—Ç–æ—Ä–∞–º
        factor_structure = {}

        for factor in factor_loadings.columns:
            # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω–æ–º—É —Ñ–∞–∫—Ç–æ—Ä—É —Å —É—á–µ—Ç–æ–º –ø–æ—Ä–æ–≥–∞
            variables = factor_loadings.index[abs(factor_loadings[factor]) >= threshold].tolist()
            factor_structure[factor] = ", ".join(variables)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ —Å—Ç—Ä–æ–∫—É

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ DataFrame
        variables_df = pd.DataFrame(list(factor_structure.items()), columns=["–§–∞–∫—Ç–æ—Ä", "–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ"])

        return variables_df


class FactorNameGenerator:
    def __init__(self, api_key='chad-9e746526a1d540a0b1d4dd56970888898jppxdhf'):
        self.api_key = api_key
        self.report_dir = Path("–û—Ç—á–µ—Ç—ã –∏ –≤—ã–≤–æ–¥—ã")

    def process(self, variables_df, communalities_df):
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö (–∏–Ω–¥–µ–∫—Å = –Ω–∞–∑–≤–∞–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö)
            description_dict = dict(zip(communalities_df.index, communalities_df['–û–ø–∏—Å–∞–Ω–∏–µ']))
            factor_prompts = []
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞
            for index, row in variables_df.iterrows():
                variables = row['–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ'].split(', ')
                descriptions = [f"{var}: {description_dict.get(var, '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ')}" for var in variables]
                prompt = (
                    f"–¢—ã —É—á–µ–Ω—ã–π –ø–æ—á–≤–æ–≤–µ–¥. –ü—Ä–∏–¥—É–º–∞–π –∫—Ä–∞—Ç–∫–æ–µ –∏ –ª–æ–≥–∏—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è —Ñ–∞–∫—Ç–æ—Ä–∞, "
                    f"—Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {', '.join(variables)}. –û–ø–∏—Å–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:\n"
                    f"{'\n'.join(descriptions)}.\n–ù–∞–∑–≤–∞–Ω–∏–µ –≤ –∫–∞–≤—ã—á–∫–∞—Ö:"
                )
                factor_prompts.append({"factor": row['–§–∞–∫—Ç–æ—Ä'], "prompt": prompt})
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API
            variables_df['–û—Ç–≤–µ—Ç'] = self._get_api_responses(factor_prompts)
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if variables_df['–û—Ç–≤–µ—Ç'].isnull().any():
                st.warning("–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")
            return variables_df

        except Exception as e:
            st.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}")
            variables_df['–û—Ç–≤–µ—Ç'] = "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
            return variables_df

    def _get_api_responses(self, factor_prompts):
        responses = []
        for item in factor_prompts:
            try:
                response = requests.post(
                    url='https://ask.chadgpt.ru/api/public/gpt-4o-mini',
                    json={"message": item["prompt"], "api_key": self.api_key}
                )
                if response.status_code == 200:
                    resp_json = response.json()
                    responses.append(resp_json['response'].strip() if resp_json['is_success'] else "–û—à–∏–±–∫–∞ API")
                else:
                    responses.append("–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è")
            except Exception as e:
                responses.append(f"–û—à–∏–±–∫–∞: {str(e)}")
        return responses
# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    pipeline = DataPipeline(
        input_file="soil_data2.xlsx",
        output_file="final_results.xlsx"
    )
    pipeline.run()
    print("\n–í—Å–µ —ç—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")

